{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638f85be-5b1d-4eec-9d3a-4bde11de02c6",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression (MLR)\n",
    "\n",
    "- Multinomial logistic regression, sometimes also called **softmax regression**\n",
    "or **maximum entropy classification**, is a method of extending logistic regression,\n",
    "which by default only works for binary classification problems, to multi-class\n",
    "classification problems.\n",
    "\n",
    "## Set up\n",
    "\n",
    "- We assume our classes are again labeled as integers, but rather than 0 and 1, if\n",
    "we have $K$ total classes, we\n",
    "will call them classes $1$ through $K$.\n",
    "\n",
    "- In our training data, though we could think of our new target $y$ values as a value\n",
    "also between 1 and $K$, we will turn $y$ into a vector of zeros and ones where the \n",
    "correct class has a value of 1 and the other entries have a value of 0 (called a \n",
    "**one-hot vector**.\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose we wish to predict a student's grade on a test from the amount of time\n",
    "they spend studying for the test (in hours) and the number of absences they had \n",
    "in class.  Assume the grade predicted will be A, B, or C (we're going to be optimistic\n",
    "about outcomes!)\n",
    "\n",
    "Our classes could be A=1, B=2, C=3 (though this particular assignment of grades\n",
    "to the class labels is arbitrary).  Imagine our data set looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d20b83-abca-4335-8100-5d72bfe3e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [['Alice', 8, 1, 'A'], ['Bob', 5, 2, 'B'], ['Carol', 6, 3, 'C'], ['Dan', 4, 1, 'B']]\n",
    "df = pd.DataFrame(data, columns=['Name', 'Hours', 'Absences', 'Grade'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e3fdb-f459-4e84-9c39-c3afa45cc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a one-hot vector using pandas \"get_dummies()\" function:\n",
    "\n",
    "one_hot = pd.get_dummies(df['Grade'])\n",
    "one_hot\n",
    "\n",
    "# Each row is a \"one-hot vector\" encoding the class for the\n",
    "# equivalent row in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb09d12-1aba-4fb5-9498-30e073250eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, Bob's vector (row 1 above) is:\n",
    "one_hot.to_numpy()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0490b7-90a4-43b4-aca5-ac1cc82b1c3a",
   "metadata": {},
   "source": [
    "Multinomial logistic regression also **predicts** vectors.\n",
    "While the input vectors to the algorithm are one-hot vectors,\n",
    "the output of the model that multinomial logistic regression creates will be **probability** vectors, in that\n",
    "the entries are interpreted to be probabilities (each between 0 and 1)\n",
    "that collectively sum to 1 (so they form a probability distribution).\n",
    "\n",
    "For example, suppose a student studies for 5 hours and has missed 3 classes.\n",
    "A probability vector might look like $[0.1, 0.45, 0.45]$, indicating our model is predicting\n",
    "that this student has a 10% chance of getting an \"A\", and a 45% chance each of getting\n",
    "a B or a C on this test.\n",
    "\n",
    "Note how the probabilities sum to 1.\n",
    "\n",
    "**In summary**, whenever we see a $\\boldsymbol{y}$ or $\\boldsymbol{\\hat{y}}$ in this\n",
    "algorithm, remember it's a vector now (with length equal to $K$, the number of classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27e561-a079-4a7c-ba10-e5de0f81860e",
   "metadata": {},
   "source": [
    "## The softmax function\n",
    "\n",
    "The softmax function is a generalization of the sigmoid/logistic function, which we\n",
    "denoted as $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$.  We will need this function when we\n",
    "define the model for multinomial logistic regression.\n",
    "\n",
    "Because we will be using the $e^x$ function a lot, and to make the equations easier\n",
    "to read, we will often use the notation $\\exp(x) = e^x$.\n",
    "\n",
    "The softmax function generalizes the sigmoid function to take a vector as input and also produces\n",
    "a vector as output.\n",
    "\n",
    "Suppose we have a vector $\\boldsymbol{z}$ with length $K$.  So $\\boldsymbol{z}=\n",
    "[z_1, z_2, \\ldots, z_K]$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{softmax}(\\boldsymbol{z}) &= \\dfrac{\\exp(z)}{\\displaystyle \\sum_{k=1}^K \\exp(z_k)}\n",
    "=\\left[ \\dfrac{\\exp(z_1)}{\\displaystyle \\sum_{k=1}^K \\exp(z_k)}, \n",
    "\\dfrac{\\exp(z_2)}{\\displaystyle \\sum_{k=1}^K \\exp(z_k)}, \\ldots, \n",
    "\\dfrac{\\exp(z_K)}{\\displaystyle \\sum_{k=1}^K \\exp(z_k)} \\right] \\\\\n",
    " &= \\dfrac{1}{\\displaystyle \\sum_{k=1}^K \\exp(z_k)}\n",
    " \\left[ \\exp(z_1), \\exp(z_2), \\ldots, \\exp(z_K) \\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "This may not look very similar to $\\dfrac{1}{1 + e^{-z}}$, but it's easier to see\n",
    "if we remember the property\n",
    "$$\\sigma(z) = \\dfrac{1}{1+e^{-z}} = \\dfrac{e^z}{e^z+1} = \\dfrac{\\exp(z)}{\\exp(z) + 1}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99eb0f-df8e-4b66-b2b4-e63fb2557f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of softmax\n",
    "\n",
    "from scipy.special import softmax\n",
    "z = [0.6, 1.1, -1.5, 1.2, 3.2, -1.1]\n",
    "\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c57dd-9a11-4b30-bafb-84bf0db62529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entries sum to 1, so this is a probability distribution.\n",
    "\n",
    "sum(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8257371f-3815-454e-8838-832b4522858a",
   "metadata": {},
   "source": [
    "## Model for multinomial logistic regression\n",
    "\n",
    "In (regular) logistic regression, recall we had   $n$ features and $m$ training examples.\n",
    " \n",
    "We also had a \n",
    "single $w$ vector:\n",
    "$\\boldsymbol{w} = [w_0, w_1, \\ldots, w_n]^T$, and our model was \n",
    "\n",
    "$$f_\\boldsymbol{w}(\\boldsymbol{x}) = \\frac{1}{1 + e^{-\\boldsymbol{w}\\cdot \\boldsymbol{x}}}$$\n",
    "\n",
    "which predicted the probability of the example $x$ being in the \"1\" class (or\n",
    "positive class).\n",
    "\n",
    "Recall that the initial number $w_0$ takes the place of the old variable $b$.\n",
    "\n",
    "In multinomial logistic regression, we will now have **separate** $\\boldsymbol{w}$ vectors for each class, from 1 to $K$.  So we will\n",
    "have a $\\boldsymbol{w}_1$ vector, and a $\\boldsymbol{w}_2$ vector, all the way up to\n",
    "a $\\boldsymbol{w}_K$ vector.\n",
    "\n",
    "Our model then becomes:\n",
    "\n",
    "$$f_\\boldsymbol{w_1, w_2, \\ldots, w_K}(x) = [\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_K]^T$$\n",
    "\n",
    "where each $\\hat{y}_k$ is defined by \n",
    "\n",
    "$$\\hat{y}_k = \\dfrac{\\exp(\\boldsymbol{w}_k \\cdot x)}{\\displaystyle \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j \\cdot x)}$$\n",
    "\n",
    "In other words, our model is outputting a vector $\\boldsymbol{\\hat{y}}$, where each\n",
    "component $\\hat{y}_k$ is determined by the equation above.\n",
    "\n",
    "### With matrices\n",
    "\n",
    "We will sometimes put all of these $\\boldsymbol{w}$ vectors into a $\\boldsymbol{W}$ matrix, where each\n",
    "row of the matrix is one of our $\\boldsymbol{w}$ vectors:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\leftarrow & \\boldsymbol{w_1} & \\rightarrow\\\\\n",
    "\\leftarrow & \\boldsymbol{w_2} & \\rightarrow\\\\\n",
    " & \\vdots & \\\\\n",
    "\\leftarrow & \\boldsymbol{w_K} & \\rightarrow\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that this matrix has $K$ rows and $n+1$ columns.\n",
    "\n",
    "We can then calculate $\\boldsymbol{\\hat{y}}$ all at once by:\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = \\text{softmax}(\\boldsymbol{W}x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e3161-fe6c-4226-bd73-9344e3f6dd51",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We now can define our loss function for MLR!  Remember the loss function is how much we\n",
    "penalize each individual training example.\n",
    "\n",
    "Recall the loss function for binary logistic regression:\n",
    "\n",
    "$$L \\left( f_\\boldsymbol{w}(\\boldsymbol{x}^{(i)}), y^{(i)} \\right) = \n",
    "        -y^{(i)}\\log\\left( f_\\boldsymbol{w}(\\boldsymbol{x}^{(i)}) \\right)-\n",
    "        (1-y^{(i)})\\log\\left( 1-f_\\boldsymbol{w}(\\boldsymbol{x}^{(i)}) \\right)$$\n",
    "        \n",
    "I'm going to simplify this and rewrite $f_\\boldsymbol{w}(\\boldsymbol{x}^{(i)})$ as just $\\hat{y}$ and remove\n",
    "all the $(i)$ parts.  So just assume $y$ is a target value for a single training example \n",
    "$\\boldsymbol{x}$ and $\\hat{y}$ is the prediction: $f_\\boldsymbol{w}(\\boldsymbol{x})$:\n",
    "\n",
    "$$L \\left( \\hat{y}, y \\right) = \n",
    "        -y\\log\\left( \\hat{y} \\right)-\n",
    "        (1-y)\\log\\left( 1-\\hat{y} \\right)$$\n",
    "        \n",
    "Remember the interpretation of this formula:  since $y$ is either 0 or 1, \n",
    "either the first or second term of this expression will be zero and drop out.\n",
    "This is because either $y$ is 0 (first term drops out) or $1-y$ is zero (second \n",
    "term drops out).\n",
    "\n",
    "This formula is sometimes called the **cross-entropy loss function**.\n",
    "\n",
    "### Extending this to MLR\n",
    "\n",
    "Notice how the formula above is basically adding up two terms, each of which is \n",
    "of the form $-\\text{something}\\log(\\hat{\\text{something}})$.\n",
    "\n",
    "We extend this idea, now remembering that $\\boldsymbol{y}$ and $\\boldsymbol{\\hat{y}}$\n",
    "are vectors:\n",
    "\n",
    "$$L( \\boldsymbol{\\hat{y}}, \\boldsymbol{y}) = -\\sum_{k=1}^K y_k \\log \\hat{y}_k$$\n",
    "\n",
    "A similar idea that happens above also happens here: all but one of these summations\n",
    "will drop out because $\\boldsymbol{y}$ is a one-hot vector.  So only one of the $y_k$\n",
    "terms above is 1; all the rest are zeros.  Let's call the $y_k$ that **is** 1 $y_c$\n",
    "($c$ standing for \"correct,\" meaning the \"correct class\"):\n",
    "\n",
    "$$L( \\boldsymbol{\\hat{y}}, \\boldsymbol{y}) = -y_c \\log \\hat{y}_c = -\\log \\hat{y}_c$$\n",
    "\n",
    "And now we can substitute in our formula above for each component of a \n",
    "$\\hat{\\boldsymbol{y}}$ vector:\n",
    "\n",
    "$$L( \\boldsymbol{\\hat{y}}, \\boldsymbol{y}) = -\\log \\hat{y}_c=\n",
    "-\\log \\dfrac{\\exp(\\boldsymbol{w}_c \\cdot \\boldsymbol{x})}{\\displaystyle \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j \\cdot \\boldsymbol{x})}$$\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b5244-1235-416b-a4b7-18a5e616351c",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "We define the cost function $J$ in the same way we always have:\n",
    "as the average of the loss function calculated across our entire training set:\n",
    "\n",
    "$$J(\\boldsymbol{w_1, w_2, \\ldots, w_K}) = J(\\boldsymbol{W}) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    f(\\boldsymbol{x}^{(i)}), \\boldsymbol{y}^{(i)} \\right) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    \\hat{\\boldsymbol{y}}^{(i)}, \\boldsymbol{y}^{(i)} \\right)$$\n",
    "    \n",
    "Where the loss function $L$ is defined as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059b28f-8216-4cdc-86b3-c7d68da31aa5",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "As always, we will use gradient descent to find the best collection of vectors\n",
    "$\\boldsymbol{w_1, w_2, \\ldots, w_K}$.\n",
    "\n",
    "We start by finding the (partial) derivative of the cost function with respect to an individual\n",
    "number within a vector $w_k$.  Recall that for (regular) logistic regression, this\n",
    "calculation was:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial w_j} J(\\boldsymbol{w}) =  \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{w}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x^{(i)}_j = \\frac{1}{m} \\sum_{i=1}^m  \\left( \\hat{y}^{(i)} - y^{(i)} \\right)  x^{(i)}_j$$\n",
    "\n",
    "Remember that the equation above is actually **multiple** equations, one per parameter $w_j$.\n",
    "\n",
    "In MLR, the major changes are that $y$ and $\\hat{y}$ are now vectors, and we now have\n",
    "multiple $w$ vectors (that we will sometimes put into a $W$ matrix).\n",
    "\n",
    "With these changes, there is now one equation per **entry** in each $w$ vector.\n",
    "The equation itself actually doesn't change that much.  For each class $k$, we have \n",
    "a vector $w_k$, and the $j$'th entry in $w_k$ we will denote by $w_{k,j}$.  The\n",
    "equation for the partial derivative of $J$ with respect to $w_{k,j}$ is:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial w_{k,j}} J(\\boldsymbol{w_1, w_2, \\ldots, w_K}) =  \n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left( \\boldsymbol{\\hat{y}}_k^{(i)} - \\boldsymbol{y}^{(i)}_k \\right)x^{(i)}_j$$\n",
    "\n",
    "with $\\boldsymbol{\\hat{y}}_k^{(i)}$ defined (copied from above) as\n",
    "\n",
    "$$\\hat{y}_k^{(i)} = \\dfrac{\\exp(\\boldsymbol{w}_k \\cdot \\boldsymbol{x}^{(i)})}{\\displaystyle \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j \\cdot \\boldsymbol{x}^{(i)})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680655e6-80ea-4374-8771-c414021547ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
